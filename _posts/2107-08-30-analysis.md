---
layout: post
title: "Analysis for trends and anomalies"
excerpt: "Analyze and visualize across multiple dimensions, automatically"
tags: [apple, mac, setup, RDP, cloud]
filename: "2107-08-30-analysis.md"
image:
  feature: https://cloud.githubusercontent.com/assets/300046/14583248/4b20c578-03d9-11e6-8f7a-c860b666bc73.jpg
  credit: Wall Street Journal
  creditlink: http://graphics.wsj.com/job-market-tracker/
comments: true
---
<i>{{ page.excerpt }}</i>
<hr />

{% include _toc.html %}

This repository is a set of software to enable processing, alerting, and visualization of data collected from various sources.

This effort is necessary for data related especially to load/stress testing because current solutions provide visualization of only individuals metrics for individual runs, without understanding additional context.

The context needed span across many dimensions to identify:

   * Creeping increases in response times for the same metric <strong>over several versions</strong> of the application.

   ![viz-pt-creeping-650x285-34125](https://user-images.githubusercontent.com/300046/30036662-8fa637d2-9172-11e7-8286-6e0fa3332fe0.jpg)

   * Sudden change in memory per user (or other ratios) within a specific enviornment, when compared against the same metric <strong>across environments</strong> from dev to production.

   ![viz-pt-env-diff-650x270-79779](https://user-images.githubusercontent.com/300046/30036768-575287fe-9173-11e7-8b2c-12eb72ada985.jpg)

   * Differences integrating data gathered from <strong>difference sources</strong> (CA-APM GC metrics, LoadRunner Transactions Per Second).

   * Differences between similar data gathered from multiple tools (from <strong>multiple vendors</strong>), such as LoadRunner, Gatling, JMeter, Selenium, etc. so that they can be evaluated for <strong>consistency</strong>.

More importantly, a different approach is needed than what current vendors provide in terms of alert triggers:  Instead of requiring people to diligently pour through all the various graphs across runs in various enviornments for every release, this effort aims for people to receive alerts after <strong>intelligent software identify trends and anomalies</strong> after assimilating data from various runs and sources, around the clock.

### Trends and Anomalies 

We want a way to use what I call "peak summary metrics", such as the amount of Memory or average response time during the "steady state period" between ramp-up and ramp-down:

   ![lr-response-time-sustained-500x208-57305](https://user-images.githubusercontent.com/300046/30041772-0d2266ae-91aa-11e7-9660-76808e9f945f.jpg)

   In the above example, when the Hits Per Second averaged 110 for the transaction at the top line (in purple), other metrics were, during the same "steady state period" period:

   * Memory averaged 3.2 GB
   * CPU averaged 65%
   * etc.
   <br /><br />

   What is different about these numbers is that each metric summarizes conditions over the entire run (not a single point in time).

Repeated over several runs, these summary numbers enables trends and anomalies to be identified in the association between average response time and sustained hits per second.

When runs are repeated several times, several sets of these last two metrics can be plotted together to yield a scatterplot such as this:

   ![viz-mem-vs-hps-500x292-32503](https://user-images.githubusercontent.com/300046/30041006-76d03876-91a3-11e7-9060-b7a9070742a6.jpg)

   The trend line calculated from various runs provide a <strong>formula</strong> of what to expect in the relationship between the two metrics.  Several other relationships can be calculated from the various measurements captured during runs.

The formula from the dotted line derived can be used proactively to identify anomalies.  During live runs the measurement system can refer to the formula to determine whether the system is functioning normally at point "p", or is using more memory than expected at point "q".

## Data Flow

   1. A performance test is run to emulate load on the application system of servers and networks.
   2. Monitoring software capture JVM dynamics such as Garbage Collection, memory and CPU used, etc.
   3. The run generatres a HTML file to summarize statistics. This is usually kept for historical review and comparison.
   4. A batch program <strong>parses HTML pages</strong> and writes the numbers from it into a database.
   5. A program analyses the accumulated data and identifies trends and anomalies.
   6. Trends and anomalies beyond a threshold cause alerts to be issued.
   7. Visualizations of the trends and anomalies are generated along with alerts about thresholds being breached.


## Artificial Intelligence

The innovation here is that 

   1) the system accumulates its data from past runs (by reading HTML files summarizing prior runs)

   2) the system tries various mechanisms to detect trends and anomalies by generating formulas and seeing how historical data fits. Different ratios are examined. Fiancial Analysts have long calculated various ratios to identify relationships among all companies, and then defining thresholds for alerts when applied to specific companies.

## Implementation

### Python

To parse HTML, I chose Python because ...

http://docs.python-guide.org/en/latest/scenarios/scrape/


