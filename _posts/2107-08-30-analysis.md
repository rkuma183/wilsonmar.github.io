---
layout: post
title: "Analysis for trends and anomalies"
excerpt: "Analyze and visualize across multiple dimensions, automatically"
tags: [apple, mac, setup, RDP, cloud]
filename: "2107-08-30-analysis.md"
image:
  feature: https://cloud.githubusercontent.com/assets/300046/14583248/4b20c578-03d9-11e6-8f7a-c860b666bc73.jpg
  credit: Wall Street Journal
  creditlink: http://graphics.wsj.com/job-market-tracker/
comments: true
---
<i>{{ page.excerpt }}</i>
<hr />

{% include _toc.html %}

This repository is a set of software to enable processing, alerting, and visualization of data collected from various sources.

This effort is necessary for data related especially to load/stress testing because current solutions provide visualization of only individuals metrics for individual runs, without understanding additional context.

The context needed span across many dimensions to identify:

   * Creeping increases in response times for the same metric <strong>over several versions</strong> of the application.

   ![viz-pt-creeping-650x285-34125](https://user-images.githubusercontent.com/300046/30036662-8fa637d2-9172-11e7-8286-6e0fa3332fe0.jpg)

   * Sudden change in memory per user (or other ratios) within a specific enviornment, when compared against the same metric <strong>across environments</strong> from dev to production.

   ![viz-pt-env-diff-650x270-79779](https://user-images.githubusercontent.com/300046/30036768-575287fe-9173-11e7-8b2c-12eb72ada985.jpg)

   * Differences integrating data gathered from <strong>difference sources</strong> (CA-APM GC metrics, LoadRunner Transactions Per Second).

   * Differences between similar data gathered from multiple tools (from <strong>multiple vendors</strong>), such as LoadRunner, Gatling, JMeter, Selenium, etc. so that they can be evaluated for <strong>consistency</strong>.

More importantly, a different approach is needed than what current vendors provide in terms of alert triggers:  Instead of requiring people to diligently pour through all the various graphs across runs in various enviornments for every release, this effort aims for people to receive alerts after <strong>intelligent software identify trends and anomalies</strong> after assimilating data from various runs and sources, around the clock.

### Trends and Anomalies 

We want a way to use what I call "peak summary metrics", such as the amount of Memory or average response time during the "steady state period" between ramp-up and ramp-down:

   ![lr-response-time-sustained-500x208-57305](https://user-images.githubusercontent.com/300046/30041772-0d2266ae-91aa-11e7-9660-76808e9f945f.jpg)

   In the above example, when the Hits Per Second averaged 110 for the transaction at the top line (in purple), other metrics were, during the same "steady state period" period:

   * Memory averaged 3.2 GB
   * CPU averaged 65%
   * etc.
   <br /><br />

   What is different about these numbers is that each metric summarizes conditions over the entire run (not a single point in time).

Repeated over several runs, sets of summary numbers enables trends and anomalies to be identified in the association between various metrics (such as between memory used and Hits per Second, etc.).

When runs are repeated several times, several sets of metrics can be plotted together to yield scatterplots such as this:

   ![viz-mem-vs-hps-500x292-32503](https://user-images.githubusercontent.com/300046/30041006-76d03876-91a3-11e7-9060-b7a9070742a6.jpg)

   The trend line calculated from various runs provide a <strong>formula</strong> of what to expect in the relationship between the two metrics.  Several other formulas (relationships) can be calculated from various combinations of measurements captured during runs.

The formula from the dotted line derived can be used to proactively identify anomalies.  During live runs the measurement system can refer to each formula to determine whether the system is functioning normally at point "p" (illustrated above), or is using more memory than expected at point "q".

### Financial ratios

Now instead of just Memory used, we calculate a ratio such as <strong>Memory per user</strong>.

   Financial Analysts have long calculated various ratios to identify relationships among all companies, and then defining thresholds for alerts when applied to specific companies.

A ratio can be more useful that a static number because it involves a relationship between two metrics.
In the case of Memory per user, history often shows that it is relatively stable for a range of HPS (Hits Per Second) levels until the HPS goes beyond a threshold level.

If in production Memory per User spikes beyond the normal level, we know there is a problem.


## Data Flow

Here is how data is processed:

   1. A performance test is run to emulate load on the application system of servers and networks.
   2. Monitoring software capture JVM dynamics such as Garbage Collection, memory and CPU used, etc.
   3. The run generatres a HTML file to summarize statistics. This is usually kept for historical review and comparison.
   4. A batch program <strong>parses HTML pages</strong> and writes the numbers from it into a database.
   5. A program analyses the accumulated data and identifies trends and anomalies (described below).
   6. Trends and anomalies in the most recent run beyond a threshold cause alerts to be issued.
   7. Visualizations of the trends and anomalies are generated along with alerts about thresholds being breached.


## Machine Learning

The innovation here is that:

   1) the system accumulates its data from past runs (by reading HTML files summarizing prior runs)

   2) the system tries various mechanisms to detect trends and anomalies by generating formulas and seeing how historical data fits. Different ratios are examined. For example, instead of Memory vs. HPS it can be CPU vs. HPS.

   3) The extent of "fit" 


## Implementation

### Python

To parse HTML, I chose Python because ...

http://docs.python-guide.org/en/latest/scenarios/scrape/


